{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifix\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchtune.modules import RotaryPositionalEmbeddings,MultiHeadAttention,RMSNorm, TransformerDecoder,KVCache\n",
    "from torchtune.models.llama3 import llama3_tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "# for pre-trained weights\n",
    "from safetensors.torch import load\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.2 1B\n",
    "\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"max_seq_length\": 131_072,  # Context length\n",
    "    \"embed_dim\": 2048,           # Embedding dimension for ecah head in self-attention\n",
    "    \"num_heads\": 32,            # Number of attention heads\n",
    "    \"head_dim\":2048 // 32,       # embed_dim // num_heads.\n",
    "    \"layers\": 16,               # Number of layers\n",
    "    \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "    \"num_kv_heads\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "'''\n",
    "# Llama 3.2 3B\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"max_seq_length\": 131_072,  # Context length\n",
    "    \"emb_dim\": 3072,           # Embedding dimension = to emb_dim in S.Raschka book\n",
    "    \"num_heads\": 24,            # Number of attention heads\n",
    "    \"head_dim\":3072 // 24,       # embed_dim // num_heads.\n",
    "    \"layers\": 28,               # Number of layers\n",
    "    \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "    \"num_kv_heads\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "'''\n",
    "LLAMA_SIZE_STR = \"1B\" if LLAMA32_CONFIG[\"embed_dim\"] == 2048 else \"3B\"\n",
    "config = LLAMA32_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message):\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }\n",
    "\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return self.tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<|begin_of_text|>': 128000, '<|end_of_text|>': 128001, '<|reserved_special_token_0|>': 128002, '<|reserved_special_token_1|>': 128003, '<|finetune_right_pad_id|>': 128004, '<|step_id|>': 128005, '<|start_header_id|>': 128006, '<|end_header_id|>': 128007, '<|eom_id|>': 128008, '<|eot_id|>': 128009, '<|python_tag|>': 128010, '<|image|>': 128256, '<|video|>': 128012, '<|reserved_special_token_2|>': 128013, '<|reserved_special_token_3|>': 128014, '<|reserved_special_token_4|>': 128015, '<|reserved_special_token_5|>': 128016, '<|reserved_special_token_6|>': 128017, '<|reserved_special_token_7|>': 128018, '<|reserved_special_token_8|>': 128019, '<|reserved_special_token_9|>': 128020, '<|reserved_special_token_10|>': 128021, '<|reserved_special_token_11|>': 128022, '<|reserved_special_token_12|>': 128023, '<|reserved_special_token_13|>': 128024, '<|reserved_special_token_14|>': 128025, '<|reserved_special_token_15|>': 128026, '<|reserved_special_token_16|>': 128027, '<|reserved_special_token_17|>': 128028, '<|reserved_special_token_18|>': 128029, '<|reserved_special_token_19|>': 128030, '<|reserved_special_token_20|>': 128031, '<|reserved_special_token_21|>': 128032, '<|reserved_special_token_22|>': 128033, '<|reserved_special_token_23|>': 128034, '<|reserved_special_token_24|>': 128035, '<|reserved_special_token_25|>': 128036, '<|reserved_special_token_26|>': 128037, '<|reserved_special_token_27|>': 128038, '<|reserved_special_token_28|>': 128039, '<|reserved_special_token_29|>': 128040, '<|reserved_special_token_30|>': 128041, '<|reserved_special_token_31|>': 128042, '<|reserved_special_token_32|>': 128043, '<|reserved_special_token_33|>': 128044, '<|reserved_special_token_34|>': 128045, '<|reserved_special_token_35|>': 128046, '<|reserved_special_token_36|>': 128047, '<|reserved_special_token_37|>': 128048, '<|reserved_special_token_38|>': 128049, '<|reserved_special_token_39|>': 128050, '<|reserved_special_token_40|>': 128051, '<|reserved_special_token_41|>': 128052, '<|reserved_special_token_42|>': 128053, '<|reserved_special_token_43|>': 128054, '<|reserved_special_token_44|>': 128055, '<|reserved_special_token_45|>': 128056, '<|reserved_special_token_46|>': 128057, '<|reserved_special_token_47|>': 128058, '<|reserved_special_token_48|>': 128059, '<|reserved_special_token_49|>': 128060, '<|reserved_special_token_50|>': 128061, '<|reserved_special_token_51|>': 128062, '<|reserved_special_token_52|>': 128063, '<|reserved_special_token_53|>': 128064, '<|reserved_special_token_54|>': 128065, '<|reserved_special_token_55|>': 128066, '<|reserved_special_token_56|>': 128067, '<|reserved_special_token_57|>': 128068, '<|reserved_special_token_58|>': 128069, '<|reserved_special_token_59|>': 128070, '<|reserved_special_token_60|>': 128071, '<|reserved_special_token_61|>': 128072, '<|reserved_special_token_62|>': 128073, '<|reserved_special_token_63|>': 128074, '<|reserved_special_token_64|>': 128075, '<|reserved_special_token_65|>': 128076, '<|reserved_special_token_66|>': 128077, '<|reserved_special_token_67|>': 128078, '<|reserved_special_token_68|>': 128079, '<|reserved_special_token_69|>': 128080, '<|reserved_special_token_70|>': 128081, '<|reserved_special_token_71|>': 128082, '<|reserved_special_token_72|>': 128083, '<|reserved_special_token_73|>': 128084, '<|reserved_special_token_74|>': 128085, '<|reserved_special_token_75|>': 128086, '<|reserved_special_token_76|>': 128087, '<|reserved_special_token_77|>': 128088, '<|reserved_special_token_78|>': 128089, '<|reserved_special_token_79|>': 128090, '<|reserved_special_token_80|>': 128091, '<|reserved_special_token_81|>': 128092, '<|reserved_special_token_82|>': 128093, '<|reserved_special_token_83|>': 128094, '<|reserved_special_token_84|>': 128095, '<|reserved_special_token_85|>': 128096, '<|reserved_special_token_86|>': 128097, '<|reserved_special_token_87|>': 128098, '<|reserved_special_token_88|>': 128099, '<|reserved_special_token_89|>': 128100, '<|reserved_special_token_90|>': 128101, '<|reserved_special_token_91|>': 128102, '<|reserved_special_token_92|>': 128103, '<|reserved_special_token_93|>': 128104, '<|reserved_special_token_94|>': 128105, '<|reserved_special_token_95|>': 128106, '<|reserved_special_token_96|>': 128107, '<|reserved_special_token_97|>': 128108, '<|reserved_special_token_98|>': 128109, '<|reserved_special_token_99|>': 128110, '<|reserved_special_token_100|>': 128111, '<|reserved_special_token_101|>': 128112, '<|reserved_special_token_102|>': 128113, '<|reserved_special_token_103|>': 128114, '<|reserved_special_token_104|>': 128115, '<|reserved_special_token_105|>': 128116, '<|reserved_special_token_106|>': 128117, '<|reserved_special_token_107|>': 128118, '<|reserved_special_token_108|>': 128119, '<|reserved_special_token_109|>': 128120, '<|reserved_special_token_110|>': 128121, '<|reserved_special_token_111|>': 128122, '<|reserved_special_token_112|>': 128123, '<|reserved_special_token_113|>': 128124, '<|reserved_special_token_114|>': 128125, '<|reserved_special_token_115|>': 128126, '<|reserved_special_token_116|>': 128127, '<|reserved_special_token_117|>': 128128, '<|reserved_special_token_118|>': 128129, '<|reserved_special_token_119|>': 128130, '<|reserved_special_token_120|>': 128131, '<|reserved_special_token_121|>': 128132, '<|reserved_special_token_122|>': 128133, '<|reserved_special_token_123|>': 128134, '<|reserved_special_token_124|>': 128135, '<|reserved_special_token_125|>': 128136, '<|reserved_special_token_126|>': 128137, '<|reserved_special_token_127|>': 128138, '<|reserved_special_token_128|>': 128139, '<|reserved_special_token_129|>': 128140, '<|reserved_special_token_130|>': 128141, '<|reserved_special_token_131|>': 128142, '<|reserved_special_token_132|>': 128143, '<|reserved_special_token_133|>': 128144, '<|reserved_special_token_134|>': 128145, '<|reserved_special_token_135|>': 128146, '<|reserved_special_token_136|>': 128147, '<|reserved_special_token_137|>': 128148, '<|reserved_special_token_138|>': 128149, '<|reserved_special_token_139|>': 128150, '<|reserved_special_token_140|>': 128151, '<|reserved_special_token_141|>': 128152, '<|reserved_special_token_142|>': 128153, '<|reserved_special_token_143|>': 128154, '<|reserved_special_token_144|>': 128155, '<|reserved_special_token_145|>': 128156, '<|reserved_special_token_146|>': 128157, '<|reserved_special_token_147|>': 128158, '<|reserved_special_token_148|>': 128159, '<|reserved_special_token_149|>': 128160, '<|reserved_special_token_150|>': 128161, '<|reserved_special_token_151|>': 128162, '<|reserved_special_token_152|>': 128163, '<|reserved_special_token_153|>': 128164, '<|reserved_special_token_154|>': 128165, '<|reserved_special_token_155|>': 128166, '<|reserved_special_token_156|>': 128167, '<|reserved_special_token_157|>': 128168, '<|reserved_special_token_158|>': 128169, '<|reserved_special_token_159|>': 128170, '<|reserved_special_token_160|>': 128171, '<|reserved_special_token_161|>': 128172, '<|reserved_special_token_162|>': 128173, '<|reserved_special_token_163|>': 128174, '<|reserved_special_token_164|>': 128175, '<|reserved_special_token_165|>': 128176, '<|reserved_special_token_166|>': 128177, '<|reserved_special_token_167|>': 128178, '<|reserved_special_token_168|>': 128179, '<|reserved_special_token_169|>': 128180, '<|reserved_special_token_170|>': 128181, '<|reserved_special_token_171|>': 128182, '<|reserved_special_token_172|>': 128183, '<|reserved_special_token_173|>': 128184, '<|reserved_special_token_174|>': 128185, '<|reserved_special_token_175|>': 128186, '<|reserved_special_token_176|>': 128187, '<|reserved_special_token_177|>': 128188, '<|reserved_special_token_178|>': 128189, '<|reserved_special_token_179|>': 128190, '<|reserved_special_token_180|>': 128191, '<|reserved_special_token_181|>': 128192, '<|reserved_special_token_182|>': 128193, '<|reserved_special_token_183|>': 128194, '<|reserved_special_token_184|>': 128195, '<|reserved_special_token_185|>': 128196, '<|reserved_special_token_186|>': 128197, '<|reserved_special_token_187|>': 128198, '<|reserved_special_token_188|>': 128199, '<|reserved_special_token_189|>': 128200, '<|reserved_special_token_190|>': 128201, '<|reserved_special_token_191|>': 128202, '<|reserved_special_token_192|>': 128203, '<|reserved_special_token_193|>': 128204, '<|reserved_special_token_194|>': 128205, '<|reserved_special_token_195|>': 128206, '<|reserved_special_token_196|>': 128207, '<|reserved_special_token_197|>': 128208, '<|reserved_special_token_198|>': 128209, '<|reserved_special_token_199|>': 128210, '<|reserved_special_token_200|>': 128211, '<|reserved_special_token_201|>': 128212, '<|reserved_special_token_202|>': 128213, '<|reserved_special_token_203|>': 128214, '<|reserved_special_token_204|>': 128215, '<|reserved_special_token_205|>': 128216, '<|reserved_special_token_206|>': 128217, '<|reserved_special_token_207|>': 128218, '<|reserved_special_token_208|>': 128219, '<|reserved_special_token_209|>': 128220, '<|reserved_special_token_210|>': 128221, '<|reserved_special_token_211|>': 128222, '<|reserved_special_token_212|>': 128223, '<|reserved_special_token_213|>': 128224, '<|reserved_special_token_214|>': 128225, '<|reserved_special_token_215|>': 128226, '<|reserved_special_token_216|>': 128227, '<|reserved_special_token_217|>': 128228, '<|reserved_special_token_218|>': 128229, '<|reserved_special_token_219|>': 128230, '<|reserved_special_token_220|>': 128231, '<|reserved_special_token_221|>': 128232, '<|reserved_special_token_222|>': 128233, '<|reserved_special_token_223|>': 128234, '<|reserved_special_token_224|>': 128235, '<|reserved_special_token_225|>': 128236, '<|reserved_special_token_226|>': 128237, '<|reserved_special_token_227|>': 128238, '<|reserved_special_token_228|>': 128239, '<|reserved_special_token_229|>': 128240, '<|reserved_special_token_230|>': 128241, '<|reserved_special_token_231|>': 128242, '<|reserved_special_token_232|>': 128243, '<|reserved_special_token_233|>': 128244, '<|reserved_special_token_234|>': 128245, '<|reserved_special_token_235|>': 128246, '<|reserved_special_token_236|>': 128247, '<|reserved_special_token_237|>': 128248, '<|reserved_special_token_238|>': 128249, '<|reserved_special_token_239|>': 128250, '<|reserved_special_token_240|>': 128251, '<|reserved_special_token_241|>': 128252, '<|reserved_special_token_242|>': 128253, '<|reserved_special_token_243|>': 128254, '<|reserved_special_token_244|>': 128255}\n",
      "[128000, 9906, 1917, 0, 128001]\n"
     ]
    }
   ],
   "source": [
    "from GPT2Model import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "\n",
    "tokenizer = llama3_tokenizer(\n",
    "    path=\"./Llama3p2_1B/tokenizer.model\"\n",
    "    )\n",
    "print(tokenizer.special_tokens)\n",
    "\n",
    "tokenized_text = tokenizer.encode(\"Hello world!\", add_bos=True, add_eos=True)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config[\"embed_dim\"],config['hidden_dim'], dtype=config[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(config[\"embed_dim\"], config['hidden_dim'], dtype=config[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(config['hidden_dim'], config[\"embed_dim\"], dtype=config[\"dtype\"], bias=False)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2245, -0.6918,  0.7142,  ...,  0.7376, -0.0289,  1.7565],\n",
      "          [ 0.6138,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
      "          [-0.5665,  0.5587,  1.8857,  ...,  0.4827,  0.8625, -0.9762],\n",
      "          [-0.8107, -0.6530,  0.2214,  ...,  0.2573,  1.4244,  1.3021],\n",
      "          [-1.2610,  0.9528, -1.1208,  ...,  0.8364,  2.1360, -1.1294]],\n",
      "\n",
      "         [[ 0.4216, -1.0776, -0.8886,  ...,  0.0750, -0.5544,  0.7537],\n",
      "          [-0.9787, -1.6997,  1.5933,  ...,  0.3558, -0.5989, -0.9587],\n",
      "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2783,  0.0923, -0.6573],\n",
      "          [-1.9951,  0.5230,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
      "          [ 1.2686, -0.9858,  0.8565,  ..., -0.7090, -1.0185,  0.4284]],\n",
      "\n",
      "         [[ 0.3841,  0.4506, -1.6809,  ...,  1.7402,  0.6556, -0.9067],\n",
      "          [-0.6769,  2.3183, -0.1740,  ..., -1.2995, -0.3551,  0.8997],\n",
      "          [ 1.1586,  0.7523,  0.8372,  ..., -0.1053,  0.2025, -0.2722],\n",
      "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
      "          [ 1.4105,  2.0177,  0.7965,  ...,  0.7452, -0.3094, -1.2789]],\n",
      "\n",
      "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
      "          [ 1.2242, -0.0030,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
      "          [ 0.3968,  0.6617, -0.2499,  ..., -0.9223, -0.0545, -0.8380],\n",
      "          [-1.7595,  0.1817, -0.7252,  ...,  0.9934, -0.0618, -0.8695],\n",
      "          [-0.1163,  0.4914, -1.0015,  ...,  0.0373,  1.2286, -0.2141]]],\n",
      "\n",
      "\n",
      "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7376, -0.0289,  1.7565],\n",
      "          [ 0.6138,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
      "          [-0.5665,  0.5587,  1.8857,  ...,  0.4827,  0.8625, -0.9762],\n",
      "          [-0.8107, -0.6530,  0.2214,  ...,  0.2573,  1.4244,  1.3021],\n",
      "          [-1.2610,  0.9528, -1.1208,  ...,  0.8364,  2.1360, -1.1294]],\n",
      "\n",
      "         [[ 0.4216, -1.0776, -0.8886,  ...,  0.0750, -0.5544,  0.7537],\n",
      "          [-0.9787, -1.6997,  1.5933,  ...,  0.3558, -0.5989, -0.9587],\n",
      "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2783,  0.0923, -0.6573],\n",
      "          [-1.9951,  0.5230,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
      "          [ 1.2686, -0.9858,  0.8565,  ..., -0.7090, -1.0185,  0.4284]],\n",
      "\n",
      "         [[ 0.3841,  0.4506, -1.6809,  ...,  1.7402,  0.6556, -0.9067],\n",
      "          [-0.6769,  2.3183, -0.1740,  ..., -1.2995, -0.3551,  0.8997],\n",
      "          [ 1.1586,  0.7523,  0.8372,  ..., -0.1053,  0.2025, -0.2722],\n",
      "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
      "          [ 1.4105,  2.0177,  0.7965,  ...,  0.7452, -0.3094, -1.2789]],\n",
      "\n",
      "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
      "          [ 1.2242, -0.0030,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
      "          [ 0.3968,  0.6617, -0.2499,  ..., -0.9223, -0.0545, -0.8380],\n",
      "          [-1.7595,  0.1817, -0.7252,  ...,  0.9934, -0.0618, -0.8695],\n",
      "          [-0.1163,  0.4914, -1.0015,  ...,  0.0373,  1.2286, -0.2141]]],\n",
      "\n",
      "\n",
      "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7376, -0.0289,  1.7565],\n",
      "          [ 0.6138,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
      "          [-0.5665,  0.5587,  1.8857,  ...,  0.4827,  0.8625, -0.9762],\n",
      "          [-0.8107, -0.6530,  0.2214,  ...,  0.2573,  1.4244,  1.3021],\n",
      "          [-1.2610,  0.9528, -1.1208,  ...,  0.8364,  2.1360, -1.1294]],\n",
      "\n",
      "         [[ 0.4216, -1.0776, -0.8886,  ...,  0.0750, -0.5544,  0.7537],\n",
      "          [-0.9787, -1.6997,  1.5933,  ...,  0.3558, -0.5989, -0.9587],\n",
      "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2783,  0.0923, -0.6573],\n",
      "          [-1.9951,  0.5230,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
      "          [ 1.2686, -0.9858,  0.8565,  ..., -0.7090, -1.0185,  0.4284]],\n",
      "\n",
      "         [[ 0.3841,  0.4506, -1.6809,  ...,  1.7402,  0.6556, -0.9067],\n",
      "          [-0.6769,  2.3183, -0.1740,  ..., -1.2995, -0.3551,  0.8997],\n",
      "          [ 1.1586,  0.7523,  0.8372,  ..., -0.1053,  0.2025, -0.2722],\n",
      "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
      "          [ 1.4105,  2.0177,  0.7965,  ...,  0.7452, -0.3094, -1.2789]],\n",
      "\n",
      "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
      "          [ 1.2242, -0.0030,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
      "          [ 0.3968,  0.6617, -0.2499,  ..., -0.9223, -0.0545, -0.8380],\n",
      "          [-1.7595,  0.1817, -0.7252,  ...,  0.9934, -0.0618, -0.8695],\n",
      "          [-0.1163,  0.4914, -1.0015,  ...,  0.0373,  1.2286, -0.2141]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2244, -0.6918,  0.7142,  ...,  0.7378, -0.0290,  1.7565],\n",
      "          [ 0.6137,  1.4071, -1.2884,  ...,  0.6648, -0.6648, -0.7109],\n",
      "          [-0.5666,  0.5586,  1.8858,  ...,  0.4827,  0.8626, -0.9762],\n",
      "          [-0.8106, -0.6530,  0.2214,  ...,  0.2571,  1.4243,  1.3022],\n",
      "          [-1.2611,  0.9527, -1.1206,  ...,  0.8365,  2.1361, -1.1292]],\n",
      "\n",
      "         [[ 0.4217, -1.0775, -0.8886,  ...,  0.0749, -0.5545,  0.7536],\n",
      "          [-0.9785, -1.6998,  1.5933,  ...,  0.3557, -0.5988, -0.9588],\n",
      "          [ 0.7588, -1.4336, -0.1143,  ..., -1.2784,  0.0923, -0.6573],\n",
      "          [-1.9951,  0.5228,  0.3864,  ...,  1.1473,  0.9758,  0.5960],\n",
      "          [ 1.2687, -0.9857,  0.8564,  ..., -0.7090, -1.0186,  0.4283]],\n",
      "\n",
      "         [[ 0.3841,  0.4507, -1.6810,  ...,  1.7402,  0.6557, -0.9067],\n",
      "          [-0.6770,  2.3182, -0.1740,  ..., -1.2996, -0.3552,  0.8997],\n",
      "          [ 1.1585,  0.7524,  0.8372,  ..., -0.1054,  0.2025, -0.2721],\n",
      "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
      "          [ 1.4104,  2.0178,  0.7965,  ...,  0.7451, -0.3093, -1.2789]],\n",
      "\n",
      "         [[-0.9674,  1.2760, -1.2678,  ...,  1.2518,  0.7012, -0.7536],\n",
      "          [ 1.2242, -0.0029,  0.4226,  ..., -0.5266,  1.2493, -0.8247],\n",
      "          [ 0.3968,  0.6617, -0.2500,  ..., -0.9223, -0.0545, -0.8380],\n",
      "          [-1.7596,  0.1816, -0.7251,  ...,  0.9933, -0.0618, -0.8695],\n",
      "          [-0.1163,  0.4914, -1.0015,  ...,  0.0372,  1.2286, -0.2140]]],\n",
      "\n",
      "\n",
      "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7377, -0.0290,  1.7565],\n",
      "          [ 0.6137,  1.4071, -1.2884,  ...,  0.6648, -0.6648, -0.7109],\n",
      "          [-0.5666,  0.5587,  1.8858,  ...,  0.4827,  0.8625, -0.9762],\n",
      "          [-0.8106, -0.6530,  0.2214,  ...,  0.2572,  1.4243,  1.3021],\n",
      "          [-1.2610,  0.9527, -1.1206,  ...,  0.8365,  2.1361, -1.1293]],\n",
      "\n",
      "         [[ 0.4216, -1.0775, -0.8886,  ...,  0.0749, -0.5545,  0.7536],\n",
      "          [-0.9786, -1.6998,  1.5933,  ...,  0.3557, -0.5988, -0.9588],\n",
      "          [ 0.7588, -1.4337, -0.1143,  ..., -1.2784,  0.0923, -0.6573],\n",
      "          [-1.9951,  0.5228,  0.3864,  ...,  1.1473,  0.9758,  0.5960],\n",
      "          [ 1.2686, -0.9857,  0.8564,  ..., -0.7090, -1.0185,  0.4283]],\n",
      "\n",
      "         [[ 0.3841,  0.4507, -1.6810,  ...,  1.7402,  0.6557, -0.9067],\n",
      "          [-0.6770,  2.3182, -0.1740,  ..., -1.2996, -0.3552,  0.8997],\n",
      "          [ 1.1585,  0.7524,  0.8372,  ..., -0.1054,  0.2025, -0.2721],\n",
      "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
      "          [ 1.4104,  2.0178,  0.7965,  ...,  0.7451, -0.3093, -1.2789]],\n",
      "\n",
      "         [[-0.9674,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
      "          [ 1.2242, -0.0029,  0.4226,  ..., -0.5266,  1.2493, -0.8247],\n",
      "          [ 0.3968,  0.6617, -0.2500,  ..., -0.9223, -0.0545, -0.8380],\n",
      "          [-1.7596,  0.1816, -0.7251,  ...,  0.9933, -0.0618, -0.8695],\n",
      "          [-0.1163,  0.4914, -1.0015,  ...,  0.0372,  1.2286, -0.2140]]],\n",
      "\n",
      "\n",
      "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7377, -0.0289,  1.7565],\n",
      "          [ 0.6137,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
      "          [-0.5665,  0.5587,  1.8858,  ...,  0.4827,  0.8625, -0.9762],\n",
      "          [-0.8106, -0.6530,  0.2214,  ...,  0.2572,  1.4243,  1.3021],\n",
      "          [-1.2610,  0.9527, -1.1207,  ...,  0.8365,  2.1360, -1.1293]],\n",
      "\n",
      "         [[ 0.4216, -1.0775, -0.8886,  ...,  0.0749, -0.5545,  0.7537],\n",
      "          [-0.9786, -1.6998,  1.5933,  ...,  0.3557, -0.5989, -0.9587],\n",
      "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2784,  0.0923, -0.6573],\n",
      "          [-1.9951,  0.5229,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
      "          [ 1.2686, -0.9858,  0.8564,  ..., -0.7090, -1.0185,  0.4283]],\n",
      "\n",
      "         [[ 0.3841,  0.4507, -1.6810,  ...,  1.7402,  0.6556, -0.9067],\n",
      "          [-0.6770,  2.3182, -0.1740,  ..., -1.2996, -0.3552,  0.8997],\n",
      "          [ 1.1585,  0.7524,  0.8372,  ..., -0.1054,  0.2025, -0.2721],\n",
      "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
      "          [ 1.4104,  2.0177,  0.7965,  ...,  0.7452, -0.3093, -1.2789]],\n",
      "\n",
      "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
      "          [ 1.2242, -0.0029,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
      "          [ 0.3968,  0.6617, -0.2500,  ..., -0.9223, -0.0545, -0.8380],\n",
      "          [-1.7595,  0.1816, -0.7251,  ...,  0.9933, -0.0618, -0.8695],\n",
      "          [-0.1163,  0.4914, -1.0015,  ...,  0.0372,  1.2286, -0.2141]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchtune.models.llama3_1._position_embeddings import Llama3ScaledRoPE\n",
    "import torch\n",
    "\n",
    "dim = config['embed_dim']\n",
    "context_len = 18\n",
    "base = 10000\n",
    "# Settings\n",
    "batch_size = 1 # Bigger than 1 does not work independently\n",
    "context_len = 5\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "# Instantiate the model\n",
    "rope = Llama3ScaledRoPE(dim = dim)  # Example dimension\n",
    "\n",
    "queries = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "\n",
    "# Apply the RoPE to the input tensor\n",
    "output_tensor = rope(queries)\n",
    "\n",
    "print(output_tensor)\n",
    "\n",
    "#help(Llama3ScaledRoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2245, -0.6918,  0.7142,  ...,  0.7376, -0.0289,  1.7565],\n",
       "          [ 0.6138,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
       "          [-0.5665,  0.5587,  1.8857,  ...,  0.4827,  0.8625, -0.9762],\n",
       "          [-0.8107, -0.6530,  0.2214,  ...,  0.2573,  1.4244,  1.3021],\n",
       "          [-1.2610,  0.9528, -1.1208,  ...,  0.8364,  2.1360, -1.1294]],\n",
       "\n",
       "         [[ 0.4216, -1.0776, -0.8886,  ...,  0.0750, -0.5544,  0.7537],\n",
       "          [-0.9787, -1.6997,  1.5933,  ...,  0.3558, -0.5989, -0.9587],\n",
       "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2783,  0.0923, -0.6573],\n",
       "          [-1.9951,  0.5230,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
       "          [ 1.2686, -0.9858,  0.8565,  ..., -0.7090, -1.0185,  0.4284]],\n",
       "\n",
       "         [[ 0.3841,  0.4506, -1.6809,  ...,  1.7402,  0.6556, -0.9067],\n",
       "          [-0.6769,  2.3183, -0.1740,  ..., -1.2995, -0.3551,  0.8997],\n",
       "          [ 1.1586,  0.7523,  0.8372,  ..., -0.1053,  0.2025, -0.2722],\n",
       "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
       "          [ 1.4105,  2.0177,  0.7965,  ...,  0.7452, -0.3094, -1.2789]],\n",
       "\n",
       "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
       "          [ 1.2242, -0.0030,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
       "          [ 0.3968,  0.6617, -0.2499,  ..., -0.9223, -0.0545, -0.8380],\n",
       "          [-1.7595,  0.1817, -0.7252,  ...,  0.9934, -0.0618, -0.8695],\n",
       "          [-0.1163,  0.4914, -1.0015,  ...,  0.0373,  1.2286, -0.2141]]],\n",
       "\n",
       "\n",
       "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7376, -0.0289,  1.7565],\n",
       "          [ 0.6138,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
       "          [-0.5665,  0.5587,  1.8857,  ...,  0.4827,  0.8625, -0.9762],\n",
       "          [-0.8107, -0.6530,  0.2214,  ...,  0.2573,  1.4244,  1.3021],\n",
       "          [-1.2610,  0.9528, -1.1208,  ...,  0.8364,  2.1360, -1.1294]],\n",
       "\n",
       "         [[ 0.4216, -1.0776, -0.8886,  ...,  0.0750, -0.5544,  0.7537],\n",
       "          [-0.9787, -1.6997,  1.5933,  ...,  0.3558, -0.5989, -0.9587],\n",
       "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2783,  0.0923, -0.6573],\n",
       "          [-1.9951,  0.5230,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
       "          [ 1.2686, -0.9858,  0.8565,  ..., -0.7090, -1.0185,  0.4284]],\n",
       "\n",
       "         [[ 0.3841,  0.4506, -1.6809,  ...,  1.7402,  0.6556, -0.9067],\n",
       "          [-0.6769,  2.3183, -0.1740,  ..., -1.2995, -0.3551,  0.8997],\n",
       "          [ 1.1586,  0.7523,  0.8372,  ..., -0.1053,  0.2025, -0.2722],\n",
       "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
       "          [ 1.4105,  2.0177,  0.7965,  ...,  0.7452, -0.3094, -1.2789]],\n",
       "\n",
       "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
       "          [ 1.2242, -0.0030,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
       "          [ 0.3968,  0.6617, -0.2499,  ..., -0.9223, -0.0545, -0.8380],\n",
       "          [-1.7595,  0.1817, -0.7252,  ...,  0.9934, -0.0618, -0.8695],\n",
       "          [-0.1163,  0.4914, -1.0015,  ...,  0.0373,  1.2286, -0.2141]]],\n",
       "\n",
       "\n",
       "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7376, -0.0289,  1.7565],\n",
       "          [ 0.6138,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
       "          [-0.5665,  0.5587,  1.8857,  ...,  0.4827,  0.8625, -0.9762],\n",
       "          [-0.8107, -0.6530,  0.2214,  ...,  0.2573,  1.4244,  1.3021],\n",
       "          [-1.2610,  0.9528, -1.1208,  ...,  0.8364,  2.1360, -1.1294]],\n",
       "\n",
       "         [[ 0.4216, -1.0776, -0.8886,  ...,  0.0750, -0.5544,  0.7537],\n",
       "          [-0.9787, -1.6997,  1.5933,  ...,  0.3558, -0.5989, -0.9587],\n",
       "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2783,  0.0923, -0.6573],\n",
       "          [-1.9951,  0.5230,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
       "          [ 1.2686, -0.9858,  0.8565,  ..., -0.7090, -1.0185,  0.4284]],\n",
       "\n",
       "         [[ 0.3841,  0.4506, -1.6809,  ...,  1.7402,  0.6556, -0.9067],\n",
       "          [-0.6769,  2.3183, -0.1740,  ..., -1.2995, -0.3551,  0.8997],\n",
       "          [ 1.1586,  0.7523,  0.8372,  ..., -0.1053,  0.2025, -0.2722],\n",
       "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
       "          [ 1.4105,  2.0177,  0.7965,  ...,  0.7452, -0.3094, -1.2789]],\n",
       "\n",
       "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
       "          [ 1.2242, -0.0030,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
       "          [ 0.3968,  0.6617, -0.2499,  ..., -0.9223, -0.0545, -0.8380],\n",
       "          [-1.7595,  0.1817, -0.7252,  ...,  0.9934, -0.0618, -0.8695],\n",
       "          [-0.1163,  0.4914, -1.0015,  ...,  0.0373,  1.2286, -0.2141]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7377, -0.0290,  1.7565],\n",
       "          [ 0.6137,  1.4071, -1.2884,  ...,  0.6648, -0.6648, -0.7109],\n",
       "          [-0.5666,  0.5587,  1.8858,  ...,  0.4827,  0.8625, -0.9762],\n",
       "          [-0.8106, -0.6530,  0.2214,  ...,  0.2572,  1.4243,  1.3021],\n",
       "          [-1.2610,  0.9527, -1.1206,  ...,  0.8365,  2.1361, -1.1293]],\n",
       "\n",
       "         [[ 0.4216, -1.0775, -0.8886,  ...,  0.0749, -0.5545,  0.7536],\n",
       "          [-0.9786, -1.6998,  1.5933,  ...,  0.3557, -0.5988, -0.9588],\n",
       "          [ 0.7588, -1.4337, -0.1143,  ..., -1.2784,  0.0923, -0.6573],\n",
       "          [-1.9951,  0.5228,  0.3864,  ...,  1.1473,  0.9758,  0.5960],\n",
       "          [ 1.2686, -0.9857,  0.8564,  ..., -0.7090, -1.0185,  0.4283]],\n",
       "\n",
       "         [[ 0.3841,  0.4507, -1.6810,  ...,  1.7402,  0.6557, -0.9067],\n",
       "          [-0.6770,  2.3182, -0.1740,  ..., -1.2996, -0.3552,  0.8997],\n",
       "          [ 1.1585,  0.7524,  0.8372,  ..., -0.1054,  0.2025, -0.2721],\n",
       "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
       "          [ 1.4104,  2.0178,  0.7965,  ...,  0.7451, -0.3093, -1.2789]],\n",
       "\n",
       "         [[-0.9674,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
       "          [ 1.2242, -0.0029,  0.4226,  ..., -0.5266,  1.2493, -0.8248],\n",
       "          [ 0.3968,  0.6617, -0.2500,  ..., -0.9223, -0.0545, -0.8380],\n",
       "          [-1.7596,  0.1816, -0.7251,  ...,  0.9933, -0.0618, -0.8695],\n",
       "          [-0.1163,  0.4914, -1.0015,  ...,  0.0372,  1.2286, -0.2141]]],\n",
       "\n",
       "\n",
       "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7377, -0.0289,  1.7565],\n",
       "          [ 0.6137,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
       "          [-0.5665,  0.5587,  1.8858,  ...,  0.4827,  0.8625, -0.9762],\n",
       "          [-0.8106, -0.6530,  0.2214,  ...,  0.2572,  1.4243,  1.3021],\n",
       "          [-1.2610,  0.9527, -1.1207,  ...,  0.8365,  2.1360, -1.1293]],\n",
       "\n",
       "         [[ 0.4216, -1.0775, -0.8886,  ...,  0.0749, -0.5545,  0.7537],\n",
       "          [-0.9786, -1.6998,  1.5933,  ...,  0.3557, -0.5989, -0.9587],\n",
       "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2783,  0.0923, -0.6573],\n",
       "          [-1.9951,  0.5229,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
       "          [ 1.2686, -0.9858,  0.8564,  ..., -0.7090, -1.0185,  0.4283]],\n",
       "\n",
       "         [[ 0.3841,  0.4507, -1.6810,  ...,  1.7402,  0.6556, -0.9067],\n",
       "          [-0.6770,  2.3182, -0.1740,  ..., -1.2996, -0.3551,  0.8997],\n",
       "          [ 1.1585,  0.7524,  0.8372,  ..., -0.1054,  0.2025, -0.2721],\n",
       "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
       "          [ 1.4104,  2.0177,  0.7965,  ...,  0.7452, -0.3093, -1.2789]],\n",
       "\n",
       "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
       "          [ 1.2242, -0.0029,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
       "          [ 0.3968,  0.6617, -0.2500,  ..., -0.9223, -0.0545, -0.8380],\n",
       "          [-1.7595,  0.1816, -0.7251,  ...,  0.9934, -0.0618, -0.8695],\n",
       "          [-0.1163,  0.4914, -1.0015,  ...,  0.0372,  1.2286, -0.2141]]],\n",
       "\n",
       "\n",
       "        [[[-0.2245, -0.6918,  0.7142,  ...,  0.7377, -0.0289,  1.7565],\n",
       "          [ 0.6137,  1.4070, -1.2883,  ...,  0.6648, -0.6648, -0.7108],\n",
       "          [-0.5665,  0.5587,  1.8857,  ...,  0.4827,  0.8625, -0.9762],\n",
       "          [-0.8107, -0.6530,  0.2214,  ...,  0.2572,  1.4243,  1.3021],\n",
       "          [-1.2610,  0.9528, -1.1207,  ...,  0.8364,  2.1360, -1.1294]],\n",
       "\n",
       "         [[ 0.4216, -1.0775, -0.8886,  ...,  0.0749, -0.5544,  0.7537],\n",
       "          [-0.9786, -1.6998,  1.5933,  ...,  0.3557, -0.5989, -0.9587],\n",
       "          [ 0.7587, -1.4337, -0.1143,  ..., -1.2783,  0.0923, -0.6573],\n",
       "          [-1.9951,  0.5229,  0.3864,  ...,  1.1472,  0.9759,  0.5960],\n",
       "          [ 1.2686, -0.9858,  0.8564,  ..., -0.7090, -1.0185,  0.4284]],\n",
       "\n",
       "         [[ 0.3841,  0.4506, -1.6810,  ...,  1.7402,  0.6556, -0.9067],\n",
       "          [-0.6769,  2.3182, -0.1740,  ..., -1.2995, -0.3551,  0.8997],\n",
       "          [ 1.1585,  0.7524,  0.8372,  ..., -0.1054,  0.2025, -0.2721],\n",
       "          [-0.2000, -0.0749,  0.3194,  ..., -1.1773,  0.1397, -0.2652],\n",
       "          [ 1.4105,  2.0177,  0.7965,  ...,  0.7452, -0.3093, -1.2789]],\n",
       "\n",
       "         [[-0.9673,  1.2761, -1.2679,  ...,  1.2518,  0.7012, -0.7536],\n",
       "          [ 1.2242, -0.0030,  0.4226,  ..., -0.5266,  1.2492, -0.8248],\n",
       "          [ 0.3968,  0.6617, -0.2499,  ..., -0.9223, -0.0545, -0.8380],\n",
       "          [-1.7595,  0.1817, -0.7251,  ...,  0.9934, -0.0618, -0.8695],\n",
       "          [-0.1163,  0.4914, -1.0015,  ...,  0.0373,  1.2286, -0.2141]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings = RotaryPositionalEmbeddings(config['embed_dim'],config['rope_base'],config['max_seq_length'])\n",
    "pos_embeddings(queries) # This works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.att =  MultiHeadAttention(\n",
    "            embed_dim=config[\"embed_dim\"],\n",
    "            max_seq_len=config[\"max_seq_length\"],\n",
    "            num_heads=config[\"num_heads\"],\n",
    "            num_kv_heads=config[\"num_kv_heads\"],\n",
    "            head_dim=config['head_dim'],\n",
    "            q_proj = nn.Linear(config['embed_dim'], config[\"embed_dim\"], bias=False, dtype=config[\"dtype\"]), \n",
    "            k_proj = nn.Linear(config['embed_dim'], config[\"num_kv_heads\"] * config['head_dim'], bias=False, dtype=config[\"dtype\"]), \n",
    "            v_proj = nn.Linear(config['embed_dim'], config[\"num_kv_heads\"] * config['head_dim'], bias=False, dtype=config[\"dtype\"]), \n",
    "            output_proj = nn.Linear(config[\"embed_dim\"], config[\"embed_dim\"], bias=False, dtype=config[\"dtype\"]),\n",
    "            pos_embeddings = Llama3ScaledRoPE(config['embed_dim'])\n",
    "        )\n",
    "        self.ff = FeedForward(config)\n",
    "        self.norm1 = RMSNorm(config[\"head_dim\"], eps=1e-06)\n",
    "        self.norm2 = RMSNorm(config[\"head_dim\"], eps=1e-06)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16))   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16))\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 128256,\n",
       " 'max_seq_length': 131072,\n",
       " 'embed_dim': 2048,\n",
       " 'num_heads': 32,\n",
       " 'head_dim': 64,\n",
       " 'layers': 16,\n",
       " 'hidden_dim': 8192,\n",
       " 'num_kv_heads': 8,\n",
       " 'rope_base': 500000.0,\n",
       " 'dtype': torch.bfloat16,\n",
       " 'rope_freq': {'factor': 32.0,\n",
       "  'low_freq_factor': 1.0,\n",
       "  'high_freq_factor': 4.0,\n",
       "  'original_context_length': 8192}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Llama3Model(\n",
       "  (tok_emb): Embedding(128256, 2048)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"embed_dim\"], dtype=config[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config[\"layers\"])])\n",
    "\n",
    "        self.final_norm = RMSNorm(config[\"embed_dim\"], eps=1e-06)\n",
    "        self.out_head = nn.Linear(config[\"embed_dim\"], config[\"vocab_size\"], bias=False, dtype=config[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(torch.bfloat16))\n",
    "        return logits\n",
    "    \n",
    "model = Llama3Model(LLAMA32_CONFIG)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,498,419,200\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of unique parameters: 1,235,750,912\n"
     ]
    }
   ],
   "source": [
    "# Account for weight tying\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling a transformer encoder from torchtune with 1B\n",
    "from torchtune.models.llama3_2 import llama3_2_1b\n",
    "\n",
    "llama32_1b = llama3_2_1b()\n",
    "llama32_1b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in llama32_1b.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262668288"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama32_1b.tok_embeddings.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2699,  1.5265, -0.4588,  ..., -1.5401,  1.0571, -2.0669],\n",
       "        [-1.7566,  0.1267, -0.1152,  ...,  1.6729,  1.3174, -0.8286],\n",
       "        [ 2.0697, -0.7493, -2.4904,  ..., -0.6723,  1.1954, -0.7240],\n",
       "        ...,\n",
       "        [ 1.3217, -0.1147,  1.1989,  ...,  0.4049,  0.7400,  2.4869],\n",
       "        [ 0.6836, -1.8381, -0.2505,  ..., -1.2482,  0.3985, -0.7639],\n",
       "        [-0.9251, -1.3305,  0.3496,  ..., -0.6322, -0.3491, -0.7308]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama32_1b.tok_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of unique parameters: 1,235,750,912\n"
     ]
    }
   ],
   "source": [
    "# Account for weight tying\n",
    "total_params_normalized = total_params - llama32_1b.tok_embeddings.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
