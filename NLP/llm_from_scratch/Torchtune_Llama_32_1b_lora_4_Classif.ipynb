{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Torchtune Llama_32_1b_lora for Classification Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchtune.modules import RotaryPositionalEmbeddings,MultiHeadAttention,RMSNorm, TransformerDecoder,KVCache\n",
    "from torchtune.models.llama3 import llama3_tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "# for pre-trained weights\n",
    "from safetensors.torch import load\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\MRM\\\\Desktop\\\\Data_Analytics\\\\Medium_and_PPB\\\\Machine_Learning\\\\Machine_Learning_Projects\\\\NLP\\\\Build_a_LLM_from_scratch\\\\llm_from_scratch'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Llama32_1B/original/tokenizer.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mllama3_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Llama32_1B/original/tokenizer.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mspecial_tokens)\n",
      "File \u001b[1;32mc:\\Users\\MRM\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torchtune\\models\\llama3\\_model_builders.py:100\u001b[0m, in \u001b[0;36mllama3_tokenizer\u001b[1;34m(path, special_tokens_path, max_seq_len, prompt_template)\u001b[0m\n\u001b[0;32m     92\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m     parse_hf_tokenizer_json(special_tokens_path)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m special_tokens_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     96\u001b[0m )\n\u001b[0;32m     97\u001b[0m template \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     98\u001b[0m     _get_prompt_template(prompt_template) \u001b[38;5;28;01mif\u001b[39;00m prompt_template \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     99\u001b[0m )\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLlama3Tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MRM\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torchtune\\models\\llama3\\_tokenizer.py:105\u001b[0m, in \u001b[0;36mLlama3Tokenizer.__init__\u001b[1;34m(self, path, special_tokens, max_seq_len, prompt_template)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# During generation, stop when either eos_id, eot_id, or eom_id is encountered\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meot_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meom_id]\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtt_model \u001b[38;5;241m=\u001b[39m \u001b[43mTikTokenBaseTokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3_tiktoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCL100K_PATTERN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m=\u001b[39m max_seq_len\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_template \u001b[38;5;241m=\u001b[39m prompt_template\n",
      "File \u001b[1;32mc:\\Users\\MRM\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torchtune\\modules\\tokenizers\\_tiktoken.py:49\u001b[0m, in \u001b[0;36mTikTokenBaseTokenizer.__init__\u001b[1;34m(self, path, name, pattern, bos_id, eos_id, special_tokens)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     42\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     special_tokens: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m     48\u001b[0m ):\n\u001b[1;32m---> 49\u001b[0m     mergeable_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtt_model \u001b[38;5;241m=\u001b[39m Encoding(\n\u001b[0;32m     51\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m     52\u001b[0m         pat_str\u001b[38;5;241m=\u001b[39mpattern,\n\u001b[0;32m     53\u001b[0m         mergeable_ranks\u001b[38;5;241m=\u001b[39mmergeable_ranks,\n\u001b[0;32m     54\u001b[0m         special_tokens\u001b[38;5;241m=\u001b[39mspecial_tokens,\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Vocab size without special tokens\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MRM\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tiktoken\\load.py:148\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[1;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     ret \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m contents\u001b[38;5;241m.\u001b[39msplitlines():\n",
      "File \u001b[1;32mc:\\Users\\MRM\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tiktoken\\load.py:63\u001b[0m, in \u001b[0;36mread_file_cached\u001b[1;34m(blobpath, expected_hash)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_hash \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_hash(contents, expected_hash):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHash mismatch for data downloaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblobpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate a corrupted download. Please try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MRM\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tiktoken\\load.py:16\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(blobpath)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblobfile is not installed. Please install it by running `pip install blobfile`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mblobfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBlobFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# avoiding blobfile for public files helps avoid auth issues, like MFA prompts\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MRM\\miniconda3\\envs\\nlp\\Lib\\site-packages\\blobfile\\_ops.py:393\u001b[0m, in \u001b[0;36mBlobFile\u001b[1;34m(path, mode, streaming, buffer_size, cache_dir, file_size, version)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBlobFile\u001b[39m(\n\u001b[0;32m    364\u001b[0m     path: RemoteOrLocalPath,\n\u001b[0;32m    365\u001b[0m     mode: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    370\u001b[0m     version: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    371\u001b[0m ):\n\u001b[0;32m    372\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    Open a local or remote file for reading or writing\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m        A file-like object\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBlobFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MRM\\miniconda3\\envs\\nlp\\Lib\\site-packages\\blobfile\\_context.py:894\u001b[0m, in \u001b[0;36mContext.BlobFile\u001b[1;34m(self, path, mode, streaming, buffer_size, cache_dir, file_size, version)\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify cache_dir for streaming files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_local_path(path):\n\u001b[1;32m--> 894\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    896\u001b[0m         f \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBufferedReader(f, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Llama32_1B/original/tokenizer.model'"
     ]
    }
   ],
   "source": [
    "tokenizer = llama3_tokenizer(\n",
    "    path=\"./Llama32_1B/original/tokenizer.model\"\n",
    "    )\n",
    "print(tokenizer.special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (dropout): Identity()\n",
       "          (lora_a): Linear(in_features=2048, out_features=8, bias=False)\n",
       "          (lora_b): Linear(in_features=8, out_features=2048, bias=False)\n",
       "        )\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): LoRALinear(\n",
       "          (dropout): Identity()\n",
       "          (lora_a): Linear(in_features=2048, out_features=8, bias=False)\n",
       "          (lora_b): Linear(in_features=8, out_features=512, bias=False)\n",
       "        )\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling a transformer encoder from torchtune with 1B\n",
    "from torchtune.models.llama3_2 import lora_llama3_2_1b, llama3_2_1b\n",
    "base_model = llama3_2_1b()\n",
    "#  [\"q_proj\", \"k_proj\", \"v_proj\", \"output_proj\"]\n",
    "latt_mods = [\"q_proj\", \"v_proj\"]\n",
    "lora_llama32_1b = lora_llama3_2_1b(lora_attn_modules=latt_mods)\n",
    "lora_llama32_1b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,236,666,368\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in lora_llama32_1b.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_llama32_1b memory footprint\n",
      "float32 (PyTorch default): 9.25 GB\n",
      "bfloat16: 4.62 GB\n",
      "\n",
      "base llama32_1b memory footprint\n",
      "float32 (PyTorch default): 9.24 GB\n",
      "bfloat16: 4.62 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print('lora_llama32_1b memory footprint')\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(lora_llama32_1b, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(lora_llama32_1b, input_dtype=torch.bfloat16):.2f} GB\")\n",
    "print('')\n",
    "print('base llama32_1b memory footprint')\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(base_model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(base_model, input_dtype=torch.bfloat16):.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load\n",
    "\n",
    "file_path = \"./Llama3p2_1B/model.safetensors\"\n",
    "with open(file_path, \"rb\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "loaded_llama = load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.layers.7.self_attn.o_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.embed_tokens.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.norm.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.15.mlp.gate_proj.weight'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_llama.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    \n",
    "    model.tok_embeddings.weight = assign(model.tok_embeddings.weight, params.embed_tokens.weight)\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # Load attention weights\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"layers.{l}.attention.wq.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"layers.{l}.attention.wk.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"layers.{l}.attention.wv.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"layers.{l}.attention.wo.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"layers.{l}.attention_norm.weight\"]\n",
    "        )\n",
    "\n",
    "        # Load FeedForward weights\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w1.weight\"]\n",
    "        )\n",
    "        # For some reason w2 and w3 are provided in the wrong order in the weights file\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w3.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w2.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"layers.{l}.ffn_norm.weight\"]\n",
    "        )\n",
    "\n",
    "    # Load output layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"norm.weight\"])\n",
    "    model.out_head.weight = assign(model.out_head.weight, params[\"output.weight\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'embed_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mload_weights_into_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_llama32_1b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_llama\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m lora_llama32_1b\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m, in \u001b[0;36mload_weights_into_llama\u001b[1;34m(model, param_config, params)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights_into_llama\u001b[39m(model, param_config, params):\n\u001b[1;32m---> 13\u001b[0m     model\u001b[38;5;241m.\u001b[39mtok_embeddings\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m assign(model\u001b[38;5;241m.\u001b[39mtok_embeddings\u001b[38;5;241m.\u001b[39mweight, \u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(param_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m         \u001b[38;5;66;03m# Load attention weights\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrf_blocks[l]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mW_query\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m assign(\n\u001b[0;32m     19\u001b[0m             model\u001b[38;5;241m.\u001b[39mtrf_blocks[l]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mW_query\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m     20\u001b[0m             params[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attention.wq.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     21\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'embed_tokens'"
     ]
    }
   ],
   "source": [
    "load_weights_into_llama(lora_llama32_1b, base_model, loaded_llama)\n",
    "lora_llama32_1b.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.attn.q_proj.lora_a.weight', 'layers.0.attn.q_proj.lora_b.weight', 'layers.0.attn.v_proj.lora_a.weight', 'layers.0.attn.v_proj.lora_b.weight', 'layers.1.attn.q_proj.lora_a.weight', 'layers.1.attn.q_proj.lora_b.weight', 'layers.1.attn.v_proj.lora_a.weight', 'layers.1.attn.v_proj.lora_b.weight', 'layers.2.attn.q_proj.lora_a.weight', 'layers.2.attn.q_proj.lora_b.weight', 'layers.2.attn.v_proj.lora_a.weight', 'layers.2.attn.v_proj.lora_b.weight', 'layers.3.attn.q_proj.lora_a.weight', 'layers.3.attn.q_proj.lora_b.weight', 'layers.3.attn.v_proj.lora_a.weight', 'layers.3.attn.v_proj.lora_b.weight', 'layers.4.attn.q_proj.lora_a.weight', 'layers.4.attn.q_proj.lora_b.weight', 'layers.4.attn.v_proj.lora_a.weight', 'layers.4.attn.v_proj.lora_b.weight', 'layers.5.attn.q_proj.lora_a.weight', 'layers.5.attn.q_proj.lora_b.weight', 'layers.5.attn.v_proj.lora_a.weight', 'layers.5.attn.v_proj.lora_b.weight', 'layers.6.attn.q_proj.lora_a.weight', 'layers.6.attn.q_proj.lora_b.weight', 'layers.6.attn.v_proj.lora_a.weight', 'layers.6.attn.v_proj.lora_b.weight', 'layers.7.attn.q_proj.lora_a.weight', 'layers.7.attn.q_proj.lora_b.weight', 'layers.7.attn.v_proj.lora_a.weight', 'layers.7.attn.v_proj.lora_b.weight', 'layers.8.attn.q_proj.lora_a.weight', 'layers.8.attn.q_proj.lora_b.weight', 'layers.8.attn.v_proj.lora_a.weight', 'layers.8.attn.v_proj.lora_b.weight', 'layers.9.attn.q_proj.lora_a.weight', 'layers.9.attn.q_proj.lora_b.weight', 'layers.9.attn.v_proj.lora_a.weight', 'layers.9.attn.v_proj.lora_b.weight', 'layers.10.attn.q_proj.lora_a.weight', 'layers.10.attn.q_proj.lora_b.weight', 'layers.10.attn.v_proj.lora_a.weight', 'layers.10.attn.v_proj.lora_b.weight', 'layers.11.attn.q_proj.lora_a.weight', 'layers.11.attn.q_proj.lora_b.weight', 'layers.11.attn.v_proj.lora_a.weight', 'layers.11.attn.v_proj.lora_b.weight', 'layers.12.attn.q_proj.lora_a.weight', 'layers.12.attn.q_proj.lora_b.weight', 'layers.12.attn.v_proj.lora_a.weight', 'layers.12.attn.v_proj.lora_b.weight', 'layers.13.attn.q_proj.lora_a.weight', 'layers.13.attn.q_proj.lora_b.weight', 'layers.13.attn.v_proj.lora_a.weight', 'layers.13.attn.v_proj.lora_b.weight', 'layers.14.attn.q_proj.lora_a.weight', 'layers.14.attn.q_proj.lora_b.weight', 'layers.14.attn.v_proj.lora_a.weight', 'layers.14.attn.v_proj.lora_b.weight', 'layers.15.attn.q_proj.lora_a.weight', 'layers.15.attn.q_proj.lora_b.weight', 'layers.15.attn.v_proj.lora_a.weight', 'layers.15.attn.v_proj.lora_b.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming that base_model already has the pretrained Llama2 weights,\n",
    "# this will directly load them into your LoRA model without any conversion necessary.\n",
    "lora_llama32_1b.load_state_dict(base_model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
