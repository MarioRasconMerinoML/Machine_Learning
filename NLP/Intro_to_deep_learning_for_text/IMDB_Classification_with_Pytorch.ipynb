{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Classification with Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "We have an already downloaded copy of the IMDB dataset, which is prepared to use with keras.utils.text_dataset_from_directory.\n",
    "\n",
    "In PyTorch, the equivalent function to Keras' text_dataset_from_directory for creating a text dataset from a directory is not available out-of-the-box. However, you can achieve similar functionality by leveraging PyTorch's data loading utilities and some custom code. Thanks to ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as op\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets.utils import check_integrity\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p torch,lightning,pandas --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextDatasetFromDir(Dataset):\n",
    "    '''\n",
    "    This class is equivalent to keras.utils.text_dataset_from_directory\n",
    "    \n",
    "    it helps to load every dataset from the pre-configured folder structure\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, root, transform=None, target_transform=None, loader=default_loader):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "        self.classes = sorted([d.name for d in os.scandir(self.root) if d.is_dir()])\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(self.root, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            with tqdm(total=12000) as pbar:\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    path = os.path.join(class_dir, filename)\n",
    "                    if not os.path.isfile(path):\n",
    "                        continue\n",
    "                    if self._has_valid_extension(filename):\n",
    "                        item = (path, self.class_to_idx[class_name])\n",
    "                        samples.append(item)\n",
    "                        pbar.update()\n",
    "        return samples\n",
    "\n",
    "    def _has_valid_extension(self, filename):\n",
    "        valid_extensions = ['.txt']  # Add more extensions if needed\n",
    "        return any(filename.endswith(ext) for ext in valid_extensions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "The default loader from Pytorch will point to the one from torchvision which at the same time will look for PIL Images,\n",
    "leading to an error.\n",
    "\n",
    "If we want to use the already downloaded IMDB dataset as it was prepared for keras, it is needed to override the loader\n",
    "of the TextDataset class we have created with the function below.\n",
    "\n",
    "'''\n",
    "def load_text(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "#root_dir = '/path/to/dataset'  # Path to the root directory of your text dataset\n",
    "train_dir = pathlib.Path('C:/Users/MRM/Desktop/Data_Analytics/Medium_and_PPB/Machine_Learning/Machine_Learning_Projects/NLP/Intro_to_deep_learning_for_text/aclImdb/train')\n",
    "val_dir = pathlib.Path('C:/Users/MRM/Desktop/Data_Analytics/Medium_and_PPB/Machine_Learning/Machine_Learning_Projects/NLP/Intro_to_deep_learning_for_text/aclImdb/val')\n",
    "test_dir = pathlib.Path('C:/Users/MRM/Desktop/Data_Analytics/Medium_and_PPB/Machine_Learning/Machine_Learning_Projects/NLP/Intro_to_deep_learning_for_text/aclImdb/test')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TextDatasetFromDir(train_dir, transform=None, loader = load_text)\n",
    "\n",
    "val_dataset = TextDatasetFromDir(val_dir, transform=None,loader = load_text)\n",
    "\n",
    "test_dataset = TextDatasetFromDir(test_dir, transform=None,loader = load_text)\n",
    "\n",
    "# Later on they will be needed other operations depending on the preprocessig approach, either BoW or Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (data, target) in enumerate(train_dataset):\n",
    "    # Do something with the data and target tensors\n",
    "    print('Train Set')\n",
    "    print(' data: ', data, ' Review: ', target[0], 'Sentiment:',  target[1])\n",
    "    if data > 3: \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also assign them to dataframes if we want to operate over them with pandas or scikit-learn\n",
    "\n",
    "review_train = []\n",
    "sent_train = []\n",
    "\n",
    "for (data, target) in enumerate(train_dataset):\n",
    "    # Do something with the data and target tensors\n",
    "    review_train.append(target[0])\n",
    "    sent_train.append(target[1])\n",
    "\n",
    "train_df= pd.DataFrame(list(zip(review_train, sent_train)), columns = ['Review', 'Sentiment'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "review_val = []\n",
    "sent_val = []\n",
    "\n",
    "for (data, target) in enumerate(val_dataset):\n",
    "    # Do something with the data and target tensors\n",
    "    review_val.append(target[0])\n",
    "    sent_val.append(target[1])\n",
    "\n",
    "val_df= pd.DataFrame(list(zip(review_val, sent_val)), columns = ['Review', 'Sentiment'])\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test = []\n",
    "sent_test = []\n",
    "\n",
    "for (data, target) in enumerate(test_dataset):\n",
    "    # Do something with the data and target tensors\n",
    "    review_test.append(target[0])\n",
    "    sent_test.append(target[1])\n",
    "\n",
    "test_df= pd.DataFrame(list(zip(review_test, sent_test)), columns = ['Review', 'Sentiment'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import portalocker\n",
    "# In order to work you must install portalocker in your environment in miniforge or conda with\n",
    "# pip install 'portalocker>=2.0.0\n",
    "train_new = IMDB(split='train')\n",
    "test_new = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in train_new:\n",
    "    print(' Review: ', example[0], ' Sentiment: ', example[1])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above in the IMBD loaded from torchtext the dataset has changed columns, in the first columns are the\n",
    "sentiment classification and in the second element of the tuple is the comment in text.\n",
    "In order to use the 'collate_batch' function below from Sebastian Raschka, some changes need to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize (find unique words) and Counter frequencies of words\n",
    "\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counts_train = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    # Standarize text. \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    # Split the words into tokens\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "# Changed from the original in the book\n",
    "# Tokenize and count on the train_dataset\n",
    "with tqdm(total=len(train_dataset)) as pbar:\n",
    "    for line, label in train_dataset:\n",
    "        tokens_train = tokenizer(line)\n",
    "        token_counts_train.update(tokens_train)\n",
    "    pbar.update()\n",
    "    \n",
    "print('Vocab-size:', len(token_counts_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same that was done with the Counter can be done with the Scikit-Learn class CountVectorizer.\n",
    "We need to extract the reviews from the train_dataset tuple to a dictionary and the keys with the text will be passed to the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_dict = dict(train_dataset)\n",
    "list(train_ds_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Convert a collection of text documents to a matrix of token counts.\n",
    "vectorizer = CountVectorizer(lowercase = True, max_features=20000, stop_words = 'english', tokenizer = tokenizer)\n",
    "\n",
    "# If we set up the max_features to 70000 we get the same vocabulary length thant with the Counter class previously used\n",
    "\n",
    "# If we don´t use stop_words we will get the same result than with the Counter class before\n",
    "#vectorizer = CountVectorizer(lowercase = True, max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(list(train_ds_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following command will give us the position of each word in the 20000 more frequent words\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding each unique token into integers\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\", 1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-A: define the functions for transformation\n",
    "device = 'cpu'\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0.\n",
    "\n",
    "\n",
    "## Step 3-B: wrap the encode and transformation function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), \n",
    "                                      dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True)\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn = collate_batch)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn = collate_batch)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn = collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (data, target) in enumerate(train_dataloader):\n",
    "    # Do something with the data and target tensors\n",
    "    print('Train Set')\n",
    "    print('Sentiment Coded: ',  target[0])\n",
    "    print('Label list Coded: ', target[1])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(train_dataloader))\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(length_batch)\n",
    "print(text_batch.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeedding Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_dict = dict(train_dataset)\n",
    "val_ds_dict = dict(val_dataset)\n",
    "test_ds_dict = dict(test_dataset)\n",
    "\n",
    "X_train = vectorizer.transform(train_ds_dict.keys())\n",
    "X_val = vectorizer.transform(val_ds_dict.keys())\n",
    "X_test = vectorizer.transform(test_ds_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(X_train[0].todense())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.bincount(np.array(X_train[0].todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_train.todense()).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
