{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipelines in Pytorch and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset is small in size that fits into our computer memory, we can perform the training in one pass.\n",
    "However normally datasets are so big that it is needed to cut them into small parts (batches) that fit into memory.\n",
    "\n",
    "Also in a normal pipeline there are some preprocessing steps like normalizing, TextVectorization, Bucketizing, Encoding, add noise, augmentation datasets etc,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "\n",
    "In Pytorch data loading, shuffling and batching is performed with DataLoader() and Dataset()\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html#dataset-types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Convert a Numpy Array, a torch tensor or a list \n",
    "# The DataLoader class allow to create batches of desired size.\n",
    "t = torch.arange(6, dtype = torch.float32)\n",
    "\n",
    "d_loader = DataLoader(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([4.])\n",
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through dataset\n",
    "\n",
    "for item in d_loader:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   tensor([0., 1.])\n",
      "1   tensor([2., 3.])\n",
      "2   tensor([4., 5.])\n",
      "3   tensor([6., 7.])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.arange(8, dtype = torch.float32)\n",
    "\n",
    "data_load = DataLoader(t2, batch_size = 2, drop_last = False)\n",
    "for i, batch in enumerate(data_load):\n",
    "    print(i, ' ', batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "tx = torch.rand([4,3], dtype = torch.float32) # Tensor for features\n",
    "ty = torch.rand([4,2], dtype = torch.float32) # Tensor for targets\n",
    "''' A map-style dataset is one that implements the __getitem__() and __len__() protocols, \n",
    "and represents a map from (possibly non-integral) indices/keys to data samples.\n",
    "\n",
    "For example, such a dataset, when accessed with dataset[idx], \n",
    "could read the idx-th image and its corresponding label from a folder on the disk.'''\n",
    "\n",
    "class JointDataset(Dataset):\n",
    "    '''\n",
    "    A custom Dataset class must contain the following methods to be used in the dataloader\n",
    "    __init__()\n",
    "    __getitem__() to return the corresponding sample to the given index\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index],self.y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x:  tensor([0.8823, 0.9150, 0.3829])  y:  tensor([0.8694, 0.5677])\n",
      " x:  tensor([0.9593, 0.3904, 0.6009])  y:  tensor([0.7411, 0.4294])\n",
      " x:  tensor([0.2566, 0.7936, 0.9408])  y:  tensor([0.8854, 0.5739])\n",
      " x:  tensor([0.1332, 0.9346, 0.5936])  y:  tensor([0.2666, 0.6274])\n"
     ]
    }
   ],
   "source": [
    "jointDataset = JointDataset(tx,ty)\n",
    "\n",
    "for example in jointDataset:\n",
    "    print(' x: ', example[0], ' y: ', example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x:  tensor(0.8823)  y:  tensor(0.9150)\n",
      " x:  tensor(0.9593)  y:  tensor(0.3904)\n",
      " x:  tensor(0.2566)  y:  tensor(0.7936)\n",
      " x:  tensor(0.1332)  y:  tensor(0.9346)\n",
      " x:  tensor(0.8694)  y:  tensor(0.5677)\n",
      " x:  tensor(0.7411)  y:  tensor(0.4294)\n",
      " x:  tensor(0.8854)  y:  tensor(0.5739)\n",
      " x:  tensor(0.2666)  y:  tensor(0.6274)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "concatDataset = ConcatDataset([tx,ty])\n",
    "for example in concatDataset:\n",
    "    print(' x: ', example[0], ' y: ', example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: x: tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.2566, 0.7936, 0.9408]]) y: tensor([[0.8694, 0.5677],\n",
      "        [0.8854, 0.5739]])\n",
      "batch 2: x: tensor([[0.1332, 0.9346, 0.5936],\n",
      "        [0.9593, 0.3904, 0.6009]]) y: tensor([[0.2666, 0.6274],\n",
      "        [0.7411, 0.4294]])\n"
     ]
    }
   ],
   "source": [
    "# Shuffle + batch + repeat in Pytorch\n",
    "\n",
    "# Shuffle. and batch size is done directly into the DataLoader class\n",
    "data_load = DataLoader(jointDataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "for i, batch in enumerate(data_load, 1):\n",
    "    print(f'batch {i}:', 'x:', batch[0], 'y:', batch[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "batch 1: x: tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.2566, 0.7936, 0.9408]]) y: tensor([[0.8694, 0.5677],\n",
      "        [0.8854, 0.5739]])\n",
      "batch 2: x: tensor([[0.1332, 0.9346, 0.5936],\n",
      "        [0.9593, 0.3904, 0.6009]]) y: tensor([[0.2666, 0.6274],\n",
      "        [0.7411, 0.4294]])\n",
      "epoch 2\n",
      "batch 1: x: tensor([[0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408]]) y: tensor([[0.7411, 0.4294],\n",
      "        [0.8854, 0.5739]])\n",
      "batch 2: x: tensor([[0.1332, 0.9346, 0.5936],\n",
      "        [0.8823, 0.9150, 0.3829]]) y: tensor([[0.2666, 0.6274],\n",
      "        [0.8694, 0.5677]])\n"
     ]
    }
   ],
   "source": [
    "# Repeating shuffling and batching on every epoch\n",
    "for epoch in range(2):\n",
    "    print(f'epoch {epoch + 1}')\n",
    "    for i, batch in enumerate(data_load, 1):\n",
    "        print(f'batch {i}:', 'x:', batch[0], 'y:', batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "In Tensorflow we use the Dataset class and the method from_tensor_slices\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/data\n",
    "\n",
    "- tf.data: Build TensorFlow input pipelines\n",
    "\n",
    "https://www.tensorflow.org/guide/data?_gl=1*16k8fhz*_ga*MTc0Mjk2NDk2Mi4xNjcxOTA0MjEy*_ga_W0YLR4190T*MTY3MTk5NzIzNi4yLjEuMTY3MTk5NzIzOS4wLjAuMA.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(6)  # any data tensor\n",
    "dataset_tf = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Iterate through dataset\n",
    "\n",
    "for item in dataset_tf:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "\n",
    "# In tensorflow we have the methods batch, repeat and shuffle \n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2], shape=(3,), dtype=int64)\n",
      "tf.Tensor([3 5 0], shape=(3,), dtype=int64)\n",
      "tf.Tensor([6 9 8], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 0 3], shape=(3,), dtype=int64)\n",
      "tf.Tensor([1 4 5], shape=(3,), dtype=int64)\n",
      "tf.Tensor([7 9 6], shape=(3,), dtype=int64)\n",
      "tf.Tensor([7 8], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(3)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 4 2], shape=(3,), dtype=int64)\n",
      "tf.Tensor([0 1 8], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Joined datasets in TF Method 1 with zip\n",
      "0 (2, 3) [1 2]\n",
      "1 (2, 3) [3 0]\n",
      "2 (2, 3) [1 3]\n",
      "3 (2, 3) [0 2]\n",
      "4 (2, 3) [2 1]\n",
      "5 (2, 3) [3 0]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "t_x = tf.random.uniform([4, 3], dtype=tf.float32)\n",
    "t_y = tf.range(4)\n",
    "\n",
    "\n",
    "ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n",
    "ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n",
    "   \n",
    "print(' Joined datasets in TF Method 1 with zip') \n",
    "ds_joint = tf.data.Dataset.zip((ds_x, ds_y))\n",
    "\n",
    "## Order 1: shuffle -> batch -> repeat\n",
    "ds = ds_joint.shuffle(4).batch(2).repeat(3)\n",
    "\n",
    "for i,(batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x.shape, batch_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (2, 3) [0 1]\n",
      "1 (2, 3) [2 3]\n",
      "2 (2, 3) [0 1]\n",
      "3 (2, 3) [2 3]\n",
      "4 (2, 3) [2 3]\n",
      "5 (2, 3) [0 1]\n"
     ]
    }
   ],
   "source": [
    "## Order 2: batch -> shuffle -> repeat\n",
    "ds = ds_joint.batch(2).shuffle(4).repeat(3)\n",
    "\n",
    "for i,(batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x.shape, batch_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Joined datasets in TF Method 1 with zip\n",
      "  x:  [0.16513085 0.9014813  0.6309742 ]   y:  0\n",
      "  x:  [0.4345461  0.29193902 0.64250207]   y:  1\n",
      "  x:  [0.9757855  0.43509948 0.6601019 ]   y:  2\n",
      "  x:  [0.60489583 0.6366315  0.6144488 ]   y:  3\n",
      " Joined datasets in TF Method 2\n",
      "  x:  [0.16513085 0.9014813  0.6309742 ]   y:  0\n",
      "  x:  [0.4345461  0.29193902 0.64250207]   y:  1\n",
      "  x:  [0.9757855  0.43509948 0.6601019 ]   y:  2\n",
      "  x:  [0.60489583 0.6366315  0.6144488 ]   y:  3\n"
     ]
    }
   ],
   "source": [
    "# Joint two Datasets in TF\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "t_x = tf.random.uniform([4, 3], dtype=tf.float32)\n",
    "t_y = tf.range(4)\n",
    "\n",
    "# Method 1\n",
    "\n",
    "ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n",
    "ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n",
    "   \n",
    "print(' Joined datasets in TF Method 1 with zip') \n",
    "ds_joint = tf.data.Dataset.zip((ds_x, ds_y))\n",
    "\n",
    "for example in ds_joint:\n",
    "    print('  x: ', example[0].numpy(), \n",
    "          '  y: ', example[1].numpy())\n",
    "    \n",
    "   \n",
    "print(' Joined datasets in TF Method 2') \n",
    "ds_joint = tf.data.Dataset.from_tensor_slices((t_x, t_y))\n",
    "\n",
    "for example in ds_joint:\n",
    "    print('  x: ', example[0].numpy(), \n",
    "          '  y: ', example[1].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlops-course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19cb93b33ae33a86621d5a429b724db7a98b1d2edee99c26e8645cea2eb5ac8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
