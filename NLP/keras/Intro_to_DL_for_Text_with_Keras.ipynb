{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO DEEP LEARNING FOR TEXT.\n",
    "\n",
    "This notebook is based on the notes taken from the following books:\n",
    "- \"Deep Learning with Python 2nd Ed\". F.Chollet\n",
    "- \"DEEP LEARNING WITH TENSORFLOW AND KERAS 3rd Ed.\n",
    "\n",
    "NLP is about to use ML and large datesets to give computers the ability to return something useful.\n",
    "Some tasks are actually, text classification, content filtering, sentiment analysis (good, bad), language modelling, \n",
    "translation, summarize, synthetize images from text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) First Step: Text Preparation\n",
    "\n",
    "Any computer needs to process text translated to numbers (Text Vectorization).\n",
    "\n",
    "- Text Vectorization Steps:\n",
    "    - Standarize (lowercase, punctuation removal)\n",
    "    - Split text in units (\"tokenize\"). \n",
    "        - Word-level\n",
    "        - N-gram tokenization\n",
    "        - Char level tokenization\n",
    "    - Convert tokens to numbers and index them into a corpus (Embeddings)\n",
    "        - [UNK] index 1 Out-ot-vocabulary index\n",
    "        - [mask] index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# A Text Vectorizer Class in pure Python\n",
    "\n",
    "import string\n",
    "\n",
    "class Vectorizer():\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {'': 0, '[UNK]': 1}\n",
    "        for text in dataset:\n",
    "            text = self.standarize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        # return tokens to words\n",
    "        self.inverse_vocabulary = dict({(v,k) for k, v in self.vocabulary.items()})\n",
    "                    \n",
    "        \n",
    "    def standarize(self, text):\n",
    "        text = text.lower()\n",
    "        returned_text = \"\".join(char for char in text if char not in string.punctuation)\n",
    "        return returned_text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        # Split into words\n",
    "        text = self.standarize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.standarize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token,1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(self.inverse_vocabulary.get(i,\"[UNK]\") for i in int_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "text4test = ['En un lugar de la Mancha de cuyo nombre no quiero acordarme', \n",
    "             'no ha mucho tiempo vivia un ingenioso hidalgo...']\n",
    "\n",
    "textVect = Vectorizer()\n",
    "textVect.make_vocabulary(text4test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = textVect.standarize(text4test[0])\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encod = textVect.encode(text4test[0])\n",
    "encod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practically in Tensorflow all these task are performed with the preprocessing layer TextVectorization\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect = TextVectorization(output_mode = 'int')\n",
    "text_vect.adapt(text4test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Word Representations:\n",
    " \n",
    " - Order Matters (RNN)\n",
    " - Sequences not ordered (Bag of Words)\n",
    " - Order agnostic (Transformer)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code only once\n",
    "# Preparation of folders strucuture that will be used later by keras.utils.text_dataset_from_directory\n",
    "\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create a batched dataset using from text_dataset_from_directory\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the dataset as a \"bag-of-words (tokens)\" --> order does not matter\n",
    "\n",
    "Individual words (Unigrams)\n",
    "A group of consecutive words N-grams\n",
    "\n",
    "Using this way by encoding words in single vectors of zeros and ones \"one-hot-encoding\". The problem is that is \n",
    "unaffordable on large corpuses. ThatÂ´s why word embeddings are preferred to manage large corpuses because they allow to compress in a low-dimensional latent space the word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keras layer TextProcessing can be used for multiple text preprocessing tasks\n",
    "\n",
    "text_vect_bow = TextVectorization(max_tokens = 10000, output_mode='multi_hot')\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vect_bow.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vect_bow(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vect_bow(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vect_bow(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Output \n",
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs shape:\", inputs.shape)\n",
    "    print(\"inputs type:\", inputs.dtype)\n",
    "    print(\"targets shape:\", targets.shape)\n",
    "    print(\"targets type:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reusable model\n",
    "from tensorflow.keras import layers\n",
    "def build_text_model(max_tokens = 10000, hidden_dims= 16):\n",
    "    inputs = keras.Input(shape = (max_tokens,))\n",
    "    # A simple dense layer\n",
    "    x = layers.Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia defines word embedding as the collective name for a set of language modeling and feature\n",
    "learning techniques in natural language processing (NLP) where words or phrases from a vocabulary\n",
    "are mapped to vectors of real numbers.\n",
    "\n",
    "Today, word embedding is a foundational technique for all kinds of NLP tasks, such as text classification, document clustering, partof-speech tagging, named entity recognition, sentiment analysis, and many more. Word embeddings\n",
    "result in dense, low-dimensional vectors, and along with LSA and topic models can be thought of as\n",
    "a vector of latent features for the word.\n",
    "\n",
    "Word embeddings are based on the distributional hypothesis, which states that words that occur in\n",
    "similar contexts tend to have similar meanings. Hence the class of word embedding-based encodings\n",
    "is also known as distributed representations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
