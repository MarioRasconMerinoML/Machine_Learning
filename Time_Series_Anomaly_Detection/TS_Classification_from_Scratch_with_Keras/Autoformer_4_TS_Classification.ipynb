{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoformer for TS Classification\n",
    "\n",
    "The Autoformer is a transformer-based model for time series forecasting, \n",
    "and its encoder part can be used for extracting features from time series data.\n",
    "\n",
    "Here's a simplified version of the Autoformer's encoder, modified for time series classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class TemporalAggregation(nn.Module):\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.mean(dim=2)\n",
    "\n",
    "class AutoformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dim_model, n_heads, num_stacks, factor):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_stacks = num_stacks\n",
    "        self.factor = factor\n",
    "\n",
    "        self.proj = nn.Conv1d(input_dim, dim_model, 1)\n",
    "        self.pos_enc = nn.Parameter(torch.randn(1, dim_model, factor))\n",
    "\n",
    "        self.stacks = nn.ModuleList(\n",
    "            [AutoformerStack(dim_model, n_heads, factor) for _ in range(num_stacks)]\n",
    "        )\n",
    "\n",
    "        self.aggregation = TemporalAggregation(dim_model)\n",
    "        self.classifier = nn.Linear(dim_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, 'b c l -> b l c')\n",
    "        x += self.pos_enc[:, :, : x.size(1)]\n",
    "\n",
    "        for stack in self.stacks:\n",
    "            x = stack(x)\n",
    "\n",
    "        x = self.aggregation(x)\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class AutoformerStack(nn.Module):\n",
    "    def __init__(self, dim_model, n_heads, factor):\n",
    "        super().__init__()\n",
    "        self.attn = AutoCorrelation(dim_model, n_heads, factor)\n",
    "        self.ff = FFN(dim_model)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class AutoCorrelation(nn.Module):\n",
    "    def __init__(self, dim_model, n_heads, factor):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.factor = factor\n",
    "        self.head_dim = dim_model // n_heads\n",
    "\n",
    "        self.qkv = nn.Conv1d(dim_model, 3 * dim_model, 1)\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, c = x.size()\n",
    "        x = self.qkv(x)\n",
    "        qkv = rearrange(x, 'b l (three c) -> three b c l', three=3)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b c l -> b l (h c)', h=self.n_heads), qkv)\n",
    "\n",
    "        # Compute auto-correlation matrix\n",
    "        k_trans = k.transpose(-2, -1)\n",
    "        r = torch.einsum('b l h d, b l h e -> b l h d e', k, k_trans) / (l - 1)\n",
    "\n",
    "        # Compute attention scores and probabilities\n",
    "        attn = torch.einsum('b l h d, b l h d e -> b l h e', q * self.scale, r)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # Compute output\n",
    "        v_mean = v.mean(dim=1, keepdim=True)\n",
    "        v_var = v.var(dim=1, keepdim=True, unbiased=False)\n",
    "        v_agg = torch.cat([v_mean, v_var], dim=-1)\n",
    "        v_agg = rearrange(v_agg, 'b 1 (h d) e -> b e (h d)')\n",
    "        out = torch.einsum('b l h e, b e h d -> b l h d', attn, v_agg)\n",
    "        out = rearrange(out, 'b l (h d) -> b l (h d)', h=self.n_heads)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Conv1d(dim_model, dim_model * 4, 1)\n",
    "        self.fc2 = nn.Conv1d(dim_model * 4, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = x1.gelu()\n",
    "        x2 = self.fc2(x1)\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1  # Number of input features\n",
    "dim_model = 512  # Model dimension\n",
    "n_heads = 8  # Number of attention heads\n",
    "num_stacks = 2  # Number of Autoformer stacks\n",
    "factor = 50  # Auto-correlation factor\n",
    "num_classes = 10  # Number of classes for classification\n",
    "\n",
    "model = AutoformerEncoder(input_dim, dim_model, n_heads, num_stacks, factor)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
