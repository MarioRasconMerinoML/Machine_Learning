{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Ensemble of TS Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class TemporalAggregation(nn.Module):\n",
    "    '''\n",
    "    In this modified version, the Informer encoder is used to extract features \n",
    "    from the input time series data, and a temporal aggregation layer is applied \n",
    "    to aggregate the features across the time dimension. Finally, a linear classifier \n",
    "    is used to predict the class labels.\n",
    "    '''\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.mean(dim=2)\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dim_model, n_heads, num_stacks, factor, distil):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_stacks = num_stacks\n",
    "        self.factor = factor\n",
    "        self.distil = distil\n",
    "\n",
    "        self.proj = nn.Conv1d(input_dim, dim_model, 1)\n",
    "        self.pos_enc = nn.Parameter(torch.randn(1, dim_model, factor))\n",
    "\n",
    "        self.stacks = nn.ModuleList(\n",
    "            [InformerStack(dim_model, n_heads, factor, distil) for _ in range(num_stacks)]\n",
    "        )\n",
    "\n",
    "        self.aggregation = TemporalAggregation(dim_model)\n",
    "        self.classifier = nn.Linear(dim_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, 'b c l -> b l c')\n",
    "        x += self.pos_enc[:, :, : x.size(1)]\n",
    "\n",
    "        for stack in self.stacks:\n",
    "            x = stack(x)\n",
    "\n",
    "        x = self.aggregation(x)\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class InformerStack(nn.Module):\n",
    "    def __init__(self, dim_model, n_heads, factor, distil):\n",
    "        super().__init__()\n",
    "        self.attn = ProbSparseSelfAttention(dim_model, n_heads, factor)\n",
    "        self.ff = FFN(dim_model, distil)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ProbSparseSelfAttention(nn.Module):\n",
    "    # ProbSparse Self-Attention mechanism\n",
    "    def __init__(self, dim_model, n_heads, factor):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.factor = factor\n",
    "        self.head_dim = dim_model // n_heads\n",
    "\n",
    "        self.qkv = nn.Conv1d(dim_model, 3 * dim_model, 1)\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, c = x.size()\n",
    "        x = self.qkv(x)\n",
    "        qkv = rearrange(x, 'b l (three c) -> three b c l', three=3)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b c l -> b l (h c)', h=self.n_heads), qkv)\n",
    "\n",
    "        # Compute attention scores and probabilities\n",
    "        attn = torch.einsum('b l h d, b l h d -> b l h', q * self.scale, k)\n",
    "        attn = attn.softmax(dim=1)\n",
    "\n",
    "        # Sample top-k indices\n",
    "        topk_indices = attn.topk(self.factor, dim=1).indices\n",
    "\n",
    "        # Gather values\n",
    "        v_gather = gather(v, 1, topk_indices)\n",
    "\n",
    "        # Compute output\n",
    "        out = torch.einsum('b l h d, b l h d -> b l h', attn[:, :, :, :self.factor], v_gather)\n",
    "        out = rearrange(out, 'b l (h d) -> b l (h d)', h=self.n_heads)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim_model, distil):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Conv1d(dim_model, dim_model * 4, 1)\n",
    "        self.fc2 = nn.Conv1d(dim_model * 4, dim_model, 1)\n",
    "        self.distil = distil\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = x1.gelu()\n",
    "        x2 = self.fc2(x1)\n",
    "        if self.distil:\n",
    "            x2 = x + x2\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class TemporalAggregation(nn.Module):\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.mean(dim=2)\n",
    "\n",
    "class AutoformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dim_model, n_heads, num_stacks, factor):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_stacks = num_stacks\n",
    "        self.factor = factor\n",
    "\n",
    "        self.proj = nn.Conv1d(input_dim, dim_model, 1)\n",
    "        self.pos_enc = nn.Parameter(torch.randn(1, dim_model, factor))\n",
    "\n",
    "        self.stacks = nn.ModuleList(\n",
    "            [AutoformerStack(dim_model, n_heads, factor) for _ in range(num_stacks)]\n",
    "        )\n",
    "\n",
    "        self.aggregation = TemporalAggregation(dim_model)\n",
    "        self.classifier = nn.Linear(dim_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, 'b c l -> b l c')\n",
    "        x += self.pos_enc[:, :, : x.size(1)]\n",
    "\n",
    "        for stack in self.stacks:\n",
    "            x = stack(x)\n",
    "\n",
    "        x = self.aggregation(x)\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class AutoformerStack(nn.Module):\n",
    "    def __init__(self, dim_model, n_heads, factor):\n",
    "        super().__init__()\n",
    "        self.attn = AutoCorrelation(dim_model, n_heads, factor)\n",
    "        self.ff = FFN(dim_model)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class AutoCorrelation(nn.Module):\n",
    "    def __init__(self, dim_model, n_heads, factor):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.factor = factor\n",
    "        self.head_dim = dim_model // n_heads\n",
    "\n",
    "        self.qkv = nn.Conv1d(dim_model, 3 * dim_model, 1)\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, c = x.size()\n",
    "        x = self.qkv(x)\n",
    "        qkv = rearrange(x, 'b l (three c) -> three b c l', three=3)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b c l -> b l (h c)', h=self.n_heads), qkv)\n",
    "\n",
    "        # Compute auto-correlation matrix\n",
    "        k_trans = k.transpose(-2, -1)\n",
    "        r = torch.einsum('b l h d, b l h e -> b l h d e', k, k_trans) / (l - 1)\n",
    "\n",
    "        # Compute attention scores and probabilities\n",
    "        attn = torch.einsum('b l h d, b l h d e -> b l h e', q * self.scale, r)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # Compute output\n",
    "        v_mean = v.mean(dim=1, keepdim=True)\n",
    "        v_var = v.var(dim=1, keepdim=True, unbiased=False)\n",
    "        v_agg = torch.cat([v_mean, v_var], dim=-1)\n",
    "        v_agg = rearrange(v_agg, 'b 1 (h d) e -> b e (h d)')\n",
    "        out = torch.einsum('b l h e, b e h d -> b l h d', attn, v_agg)\n",
    "        out = rearrange(out, 'b l (h d) -> b l (h d)', h=self.n_heads)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Conv1d(dim_model, dim_model * 4, 1)\n",
    "        self.fc2 = nn.Conv1d(dim_model * 4, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = x1.gelu()\n",
    "        x2 = self.fc2(x1)\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class TemporalAggregation(nn.Module):\n",
    "    '''\n",
    "    In this modified version, the TFT encoder is used to extract features from the input time series data, \n",
    "    and a temporal aggregation layer is applied to aggregate the features across the time dimension. \n",
    "    Finally, a linear classifier is used to predict the class labels.\n",
    "    '''\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.mean(dim=2)\n",
    "\n",
    "class TFTEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dim_model, n_heads, num_stacks, factor):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_stacks = num_stacks\n",
    "        self.factor = factor\n",
    "\n",
    "        self.proj = nn.Conv1d(input_dim, dim_model, 1)\n",
    "        self.pos_enc = nn.Parameter(torch.randn(1, dim_model, factor))\n",
    "\n",
    "        self.stacks = nn.ModuleList(\n",
    "            [TFTStack(dim_model, n_heads, factor) for _ in range(num_stacks)]\n",
    "        )\n",
    "\n",
    "        self.aggregation = TemporalAggregation(dim_model)\n",
    "        self.classifier = nn.Linear(dim_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, 'b c l -> b l c')\n",
    "        x += self.pos_enc[:, :, : x.size(1)]\n",
    "\n",
    "        for stack in self.stacks:\n",
    "            x = stack(x)\n",
    "\n",
    "        x = self.aggregation(x)\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TFTStack(nn.Module):\n",
    "    def __init__(self, dim_model, n_heads, factor):\n",
    "        super().__init__()\n",
    "        self.attn = TFTAttention(dim_model, n_heads, factor)\n",
    "        self.ff = FFN(dim_model)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class TFTAttention(nn.Module):\n",
    "    def __init__(self, dim_model, n_heads, factor):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.factor = factor\n",
    "        self.head_dim = dim_model // n_heads\n",
    "\n",
    "        self.qkv = nn.Conv1d(dim_model, 3 * dim_model, 1)\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.gating = nn.Linear(dim_model, 1)\n",
    "        self.static_conv = nn.Conv1d(dim_model, dim_model, 1)\n",
    "        self.static_proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, c = x.size()\n",
    "        x = self.qkv(x)\n",
    "        qkv = rearrange(x, 'b l (three c) -> three b c l', three=3)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b c l -> b l (h c)', h=self.n_heads), qkv)\n",
    "\n",
    "        # Compute attention scores and probabilities\n",
    "        attn = torch.einsum('b l h d, b l h e -> b l h d e', q * self.scale, k.transpose(-2, -1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # Compute static and temporal components\n",
    "        static_comp = self.static_conv(x)\n",
    "        static_comp = rearrange(static_comp, 'b l (h d) -> b l h d', h=self.n_heads)\n",
    "        static_comp = self.static_proj(static_comp.mean(dim=1))\n",
    "        static_comp = rearrange(static_comp, 'b l (h d) -> b l h d', h=self.n_heads)\n",
    "\n",
    "        temporal_comp = torch.einsum('b l h d e, b l h d -> b l h d', attn, v)\n",
    "\n",
    "        # Apply gating mechanism\n",
    "        gate = self.gating(x).sigmoid()\n",
    "        gate = rearrange(gate, 'b l (h d) -> b l h d', h=self.n_heads)\n",
    "        out = gate * static_comp + (1 - gate) * temporal_comp\n",
    "\n",
    "        out = rearrange(out, 'b l (h d) -> b l (h d)', h=self.n_heads)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Conv1d(dim_model, dim_model * 4, 1)\n",
    "        self.fc2 = nn.Conv1d(dim_model * 4, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = x1.gelu()\n",
    "        x2 = self.fc2(x1)\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate models\n",
    "input_dim = 1\n",
    "dim_model = 512\n",
    "n_heads = 8\n",
    "num_stacks = 2\n",
    "factor = 50\n",
    "num_classes = 10\n",
    "\n",
    "informer_model = InformerEncoder(input_dim, dim_model, n_heads, num_stacks, factor, True)\n",
    "autformer_model = AutoformerEncoder(input_dim, dim_model, n_heads, num_stacks, factor)\n",
    "tft_model = TFTEncoder(input_dim, dim_model, n_heads, num_stacks, factor)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_informer = optim.Adam(informer_model.parameters())\n",
    "optimizer_autformer = optim.Adam(autformer_model.parameters())\n",
    "optimizer_tft = optim.Adam(tft_model.parameters())\n",
    "\n",
    "# Train models\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train Informer\n",
    "    informer_model.train()\n",
    "    optimizer_informer.zero_grad()\n",
    "    informer_output = informer_model(input_data)\n",
    "    loss_informer = criterion(informer_output, target_data)\n",
    "    loss_informer.backward()\n",
    "    optimizer_informer.step()\n",
    "\n",
    "    # Train Autoformer\n",
    "    autformer_model.train()\n",
    "    optimizer_autformer.zero_grad()\n",
    "    autformer_output = autformer_model(input_data)\n",
    "    loss_autformer = criterion(autformer_output, target_data)\n",
    "    loss_autformer.backward()\n",
    "    optimizer_autformer.step()\n",
    "\n",
    "    # Train TFT\n",
    "    tft_model.train()\n",
    "    optimizer_tft.zero_grad()\n",
    "    tft_output = tft_model(input_data)\n",
    "    loss_tft = criterion(tft_output, target_data)\n",
    "    loss_tft.backward()\n",
    "    optimizer_tft.step()\n",
    "\n",
    "# Evaluate ensemble\n",
    "informer_model.eval()\n",
    "autformer_model.eval()\n",
    "tft_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    informer_output = informer_model(test_data)\n",
    "    autformer_output = autformer_model(test_data)\n",
    "    tft_output = tft_model(test_data)\n",
    "\n",
    "    # Combine predictions using soft voting\n",
    "    ensemble_logits = (informer_output + autformer_output + tft_output) / 3\n",
    "    ensemble_probs = nn.functional.softmax(ensemble_logits, dim=1)\n",
    "    ensemble_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_labels, ensemble_preds.cpu().numpy())\n",
    "    print(f\"Ensemble accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
