{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the encoder part of the Informer model for time series classification\n",
    "\n",
    "The Informer is a transformer-based model for time series forecasting, and its encoder part can be used for extracting features from time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class TemporalAggregation(nn.Module):\n",
    "    '''\n",
    "    In this modified version, the Informer encoder is used to extract features \n",
    "    from the input time series data, and a temporal aggregation layer is applied \n",
    "    to aggregate the features across the time dimension. Finally, a linear classifier \n",
    "    is used to predict the class labels.\n",
    "    '''\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.mean(dim=2)\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, dim_model, n_heads, num_stacks, factor, distil):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_stacks = num_stacks\n",
    "        self.factor = factor\n",
    "        self.distil = distil\n",
    "\n",
    "        self.proj = nn.Conv1d(input_dim, dim_model, 1)\n",
    "        self.pos_enc = nn.Parameter(torch.randn(1, dim_model, factor))\n",
    "\n",
    "        self.stacks = nn.ModuleList(\n",
    "            [InformerStack(dim_model, n_heads, factor, distil) for _ in range(num_stacks)]\n",
    "        )\n",
    "\n",
    "        self.aggregation = TemporalAggregation(dim_model)\n",
    "        self.classifier = nn.Linear(dim_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, 'b c l -> b l c')\n",
    "        x += self.pos_enc[:, :, : x.size(1)]\n",
    "\n",
    "        for stack in self.stacks:\n",
    "            x = stack(x)\n",
    "\n",
    "        x = self.aggregation(x)\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class InformerStack(nn.Module):\n",
    "    def __init__(self, dim_model, n_heads, factor, distil):\n",
    "        super().__init__()\n",
    "        self.attn = ProbSparseSelfAttention(dim_model, n_heads, factor)\n",
    "        self.ff = FFN(dim_model, distil)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ProbSparseSelfAttention(nn.Module):\n",
    "    # ProbSparse Self-Attention mechanism\n",
    "    def __init__(self, dim_model, n_heads, factor):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.n_heads = n_heads\n",
    "        self.factor = factor\n",
    "        self.head_dim = dim_model // n_heads\n",
    "\n",
    "        self.qkv = nn.Conv1d(dim_model, 3 * dim_model, 1)\n",
    "        self.proj = nn.Conv1d(dim_model, dim_model, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, c = x.size()\n",
    "        x = self.qkv(x)\n",
    "        qkv = rearrange(x, 'b l (three c) -> three b c l', three=3)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b c l -> b l (h c)', h=self.n_heads), qkv)\n",
    "\n",
    "        # Compute attention scores and probabilities\n",
    "        attn = torch.einsum('b l h d, b l h d -> b l h', q * self.scale, k)\n",
    "        attn = attn.softmax(dim=1)\n",
    "\n",
    "        # Sample top-k indices\n",
    "        topk_indices = attn.topk(self.factor, dim=1).indices\n",
    "\n",
    "        # Gather values\n",
    "        v_gather = gather(v, 1, topk_indices)\n",
    "\n",
    "        # Compute output\n",
    "        out = torch.einsum('b l h d, b l h d -> b l h', attn[:, :, :, :self.factor], v_gather)\n",
    "        out = rearrange(out, 'b l (h d) -> b l (h d)', h=self.n_heads)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim_model, distil):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Conv1d(dim_model, dim_model * 4, 1)\n",
    "        self.fc2 = nn.Conv1d(dim_model * 4, dim_model, 1)\n",
    "        self.distil = distil\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = x1.gelu()\n",
    "        x2 = self.fc2(x1)\n",
    "        if self.distil:\n",
    "            x2 = x + x2\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1  # Number of input features\n",
    "dim_model = 512  # Model dimension\n",
    "n_heads = 8  # Number of attention heads\n",
    "num_stacks = 2  # Number of Informer stacks\n",
    "factor = 50  # ProbSparse attention factor\n",
    "distil = True  # Whether to use distillation in FFN\n",
    "num_classes = 10  # Number of classes for classification\n",
    "\n",
    "model = InformerEncoder(input_dim, dim_model, n_heads, num_stacks, factor, distil)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
